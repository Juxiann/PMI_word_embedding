{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from numba import njit, prange\n",
    "import time, pickle, os\n",
    "from scipy.sparse.linalg import svds\n",
    "from scipy.sparse import csr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\juxia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"wiki-text.txt\", 'r') as file:\n",
    "    wc = file.read()\n",
    "wc = wc.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124301826"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english']\n"
     ]
    }
   ],
   "source": [
    "print(wc[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter volcabulary\n",
    "-Remove words that appear less than n times (here n = 500). \n",
    "\n",
    "-Remove all words that appear in the stopwords list of nltk package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_count = Counter(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13201"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict = {}\n",
    "for vocab in wc_count:\n",
    "    if wc_count[vocab] > 500 and vocab not in stop_words:\n",
    "        vocab_dict[vocab] = wc_count[vocab]\n",
    "d = len(vocab_dict)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove words not in vocabulary\n",
    "def remove_excess_words(wc):\n",
    "    removed = []\n",
    "    for word in wc:\n",
    "        if word in vocab_dict:\n",
    "            removed.append(word)\n",
    "    return removed        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_wc = remove_excess_words(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'term', 'abuse', 'first', 'used', 'early', 'working', 'class', 'radicals', 'including', 'english', 'revolution', 'sans', 'french', 'revolution', 'whilst', 'term', 'still', 'used']\n"
     ]
    }
   ],
   "source": [
    "print(removed_wc[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71618337"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(removed_wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up memory\n",
    "del wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dictionary\n",
    "# pickle.dump(removed_wc, open(\"removed_wc.p\", \"wb\" ) )\n",
    "pickle.dump(vocab_dict, open(\"vocab_dict.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dictionary\n",
    "# removed_wc = pickle.load(open(\"removed_wc.p\", \"rb\" ) )\n",
    "vocab_dict = pickle.load(open(\"vocab_dict.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute PMI Matrix\n",
    "$\n",
    "M_{ij} = \\log\\left( \\frac{(N^p(w_i, w_j)+1)\\cdot N(S^p)}{N^p(w_i)\\cdot N^p(w_j)} \\right)\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vocabulary index look-up table. \n",
    "# We exploit the fact that dict.keys are ordered in Python 3.\n",
    "def return_vilut(vocab_dict):\n",
    "    vocab_index_look_up_table = {}\n",
    "    vocab_list = list(vocab_dict.keys())\n",
    "    for i in range(len(vocab_dict.keys())):\n",
    "        vocab_index_look_up_table[vocab_list[i]] = i\n",
    "    return vocab_index_look_up_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('anarchism', 0), ('originated', 1), ('term', 2), ('abuse', 3), ('first', 4)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vilut = return_vilut(vocab_dict)\n",
    "\n",
    "# Sample look-up table, real look up table is a dict\n",
    "[(i, vilut[i]) for i in list(vilut.keys())[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word_occurences(wc, vilut):\n",
    "    size = 2\n",
    "    d = len(vilut)\n",
    "    n = len(wc)\n",
    "    # N^p(w_i, w_j)\n",
    "    word_pair_count = np.zeros((d, d),dtype=\"int64\")\n",
    "    # N^p(w)\n",
    "    word_count = np.zeros(d,dtype=\"int64\")\n",
    "    # N(S^p)\n",
    "    NSp = 0\n",
    "    t1 = time.time()\n",
    "    for i in range(n):\n",
    "        if i%10**7==0:\n",
    "            print(f\"{i}/{n} completed, {round(time.time()-t1, 2)}s elapsed\")\n",
    "        neighbours = list(range(max(0, i-size), min(i+size+1, n)))\n",
    "        neighbours.remove(i)\n",
    "        for j in neighbours:\n",
    "            ind_i = vilut[wc[i]]\n",
    "            ind_j = vilut[wc[j]]\n",
    "            word_pair_count[ind_i, ind_j] += 1\n",
    "            word_count[ind_j] += 1\n",
    "            NSp += 1\n",
    "    return word_count, word_pair_count, NSp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/71618337 completed, 0.0s elapsed\n",
      "10000000/71618337 completed, 55.39s elapsed\n",
      "20000000/71618337 completed, 107.81s elapsed\n",
      "30000000/71618337 completed, 161.29s elapsed\n",
      "40000000/71618337 completed, 210.53s elapsed\n",
      "50000000/71618337 completed, 261.34s elapsed\n",
      "60000000/71618337 completed, 313.4s elapsed\n",
      "70000000/71618337 completed, 365.29s elapsed\n"
     ]
    }
   ],
   "source": [
    "word_count, word_pair_count, NSp = count_word_occurences(removed_wc,vilut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.log((word_pair_count+1) * NSp / (word_count.reshape(-1,1) @ word_count.reshape(1,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the precious data\n",
    "pickle.dump(word_pair_count, open(\"word_pair_count.p\", \"wb\" ))\n",
    "pickle.dump(word_count, open(\"word_count.p\", \"wb\" ) )\n",
    "pickle.dump(M, open(\"M.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.77524551,  2.66743035,  1.76739777, ...,  3.28795218,\n",
       "         1.78547095,  2.47968366],\n",
       "       [ 2.66743035,  1.71458438,  3.03375528, ...,  1.92964109,\n",
       "         0.42715986,  1.12137257],\n",
       "       [ 1.76739777,  3.03375528,  2.31725125, ..., -0.5798294 ,\n",
       "        -2.08231063, -0.69495074],\n",
       "       ...,\n",
       "       [ 3.28795218,  1.92964109, -0.5798294 , ...,  3.24331009,\n",
       "         1.74082887,  2.43504158],\n",
       "       [ 1.78547095,  0.42715986, -2.08231063, ...,  1.74082887,\n",
       "         0.23834764,  0.93256035],\n",
       "       [ 2.47968366,  1.12137257, -0.69495074, ...,  2.43504158,\n",
       "         0.93256035,  1.62677306]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factorize the PMI matrix to obtain word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "M = pickle.load(open(\"M.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, s, V =svds(M, k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = U @ np.diag(s**(1/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13201, 50)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "pickle.dump(W, open(\"W.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application: Find closest word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(W, vocab_dict, vilut, word, n):\n",
    "    \"\"\"\n",
    "    Returns n words which appear around word with highest probability.\"\"\"\n",
    "    v_w = W[vilut[word]]\n",
    "    n_max_ind = np.argsort(-W @ v_w)[:n]\n",
    "    vocab = list(vocab_dict.keys())\n",
    "    return [vocab[i] for i in n_max_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['physics', 'science', 'mathematics', 'theory', 'mathematical']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_context(W, vocab_dict, vilut, \"physics\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['republican', 'president', 'election', 'party', 'democratic']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_context(W, vocab_dict, vilut, \"republican\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lorentz', 'planck', 'leibniz', 'gauss', 'einstein']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_context(W, vocab_dict, vilut, \"einstein\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['theorem', 'algebra', 'frac', 'finite', 'vector']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_context(W, vocab_dict, vilut, \"algebra\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['species', 'food', 'fish', 'fruit', 'plants']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_context(W, vocab_dict, vilut, \"fish\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application: Solve analogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_analogies(X, Y, Z, W, vocab_dict, vilut,n):\n",
    "    \"\"\"\n",
    "    Returns n words that solve the analogy X : Y | Z : ? \n",
    "    \"\"\"\n",
    "    v_w = W[vilut[Y]] - W[vilut[X]] + W[vilut[Z]]\n",
    "    n_max_ind = np.argsort(-W @ v_w)[:n]\n",
    "    vocab = list(vocab_dict.keys())\n",
    "    return [vocab[i] for i in n_max_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['conservative', 'liberal', 'conservatives', 'democrats', 'views']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solve_analogies(\"republican\", \"democrat\", \"conservative\", W, vocab_dict, vilut,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['london', 'england', 'st', 'college', 'york']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solve_analogies(\"france\", \"paris\", \"england\", W, vocab_dict, vilut,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['river', 'north', 'located', 'south', 'west']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solve_analogies(\"car\", \"road\", \"boat\", W, vocab_dict, vilut,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['soviet', 'german', 'forces', 'military', 'communist']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solve_analogies(\"america\", \"capitalist\", \"soviet\", W, vocab_dict, vilut,5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
